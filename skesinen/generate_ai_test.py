import pandas as pd
import random
from datasets import load_dataset

OUTPUT_FILE = "liar_isot/data/long_ai_test_100.csv"

def main():
    print("--- GENERATING 100 REAL vs AI-FAKE TEST CASES ---")
    
    # 1. Get 50 REAL Long Articles (CNN/DailyMail)
    print("Downloading Real News (CNN/DailyMail)...")
    try:
        # We use a different split/slice to ensure variety
        dataset_real = load_dataset("cnn_dailymail", '3.0.0', split="train[:500]")
    except:
        print("Error downloading CNN. Check internet.")
        return
    
    real_list = []
    print("Filtering Real news for length > 300 words...")
    for item in dataset_real:
        text = item['article']
        if len(text.split()) > 300:
            real_list.append({'text': text, 'label': 1}) # 1 = Real
            if len(real_list) >= 50: break

    # 2. Get 50 FAKE (AI-Generated) Articles
    print("Downloading AI Fake News (skeskinen/generated-fake-news)...")
    try:
        # This dataset contains text generated by models like GPT-J
        dataset_fake = load_dataset("skeskinen/generated-fake-news", split="train")
    except:
        print("Error downloading AI dataset.")
        return

    fake_list = []
    print("Filtering AI news for length > 300 words...")
    for item in dataset_fake:
        # Depending on the dataset structure, text key might vary
        # This specific dataset usually has 'text' and 'label' (1=fake usually)
        # We force label 0 for our model
        text = item['text']
        
        if len(text.split()) > 300:
            fake_list.append({'text': text, 'label': 0}) # 0 = Fake
            if len(fake_list) >= 50: break

    # 3. Combine and Save
    full_data = real_list + fake_list
    random.shuffle(full_data)
    
    df = pd.DataFrame(full_data)
    df.to_csv(OUTPUT_FILE, index=False)
    
    print(f"\nâœ… Created {OUTPUT_FILE}")
    print(f"Real Samples: {len(real_list)}")
    print(f"Fake Samples: {len(fake_list)}")

if __name__ == "__main__":
    main()